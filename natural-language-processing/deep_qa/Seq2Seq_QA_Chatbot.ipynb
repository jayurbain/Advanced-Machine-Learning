{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documentation for Seq2Seq QA Chatbot\n",
    "\n",
    "Honors assignment for Advanced Machine Learning - Natural Language Processing\n",
    "\n",
    "Jay Urbain, jay.urbain@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "\n",
    "Extension of StackOverlow Telegram Chatbot for the final project of the Natural Language Processing course using a 2-layer sequence to sequence (encoder-decoder) LSTM network. Trained on the Cornell Movie Dialogues corpus and the OpenSubtitles corpus.\n",
    "\n",
    "Run using Telegram chatbot:    \n",
    "@jay_iqabot   \n",
    "bot name: iQABot  \n",
    "\n",
    "References incuded below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements for the report (IPython notebook):\n",
    "\n",
    "#### Describe the chosen approach and implementation details in plain English. The code is not enough!\n",
    "\n",
    "Extension of StackOverlow Telegram Chatbot for the final project of the Natural Language Processing course.\n",
    "Uses a 2-layer sequence to sequence (encoder-decoder) LSTM network for question and answer dialogue generation.\n",
    "\n",
    "ChatterBot based code was modified to handle the question answering for new deep_qa question-answering dialogue generation. In the root directory of deep_qa, the following ChatterBot-based files were modified:\n",
    "- main_bot.py (a new QADialogueManager was defined directly in main_bot.py) \n",
    "\n",
    "DeepQA based code was modified to allow the ChatterBot based code to open a TensorFlow based session, send questions and receive answers, and close the session. Specific changes can be found by search for 'jay'     \n",
    "See deep_qa/chatbot.py: \n",
    "- def telecomChatBotSessionOpen(self, args=None)  \n",
    "- def telecomChatBotSessionQuery(self, question)  \n",
    "- def telecomChatBotSessionClose(self, question)  \n",
    "\n",
    "2-layer LSTM encoder-decoder seq2seq model update for DeepQA  \n",
    "- deep_qa/model.py\n",
    "\n",
    "##### Training data:         \n",
    "Cornell Movie Dialogs - small corpus of dialogues from English movies\n",
    "http://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html)\n",
    "\n",
    "OpenSubtitles - bigger corpus of English subtitles (but also noisier).\n",
    "http://opus.nlpl.eu/OpenSubtitles.php\n",
    "\n",
    "##### Training  \n",
    "\n",
    "mkdir data # from deep_qa  \n",
    "// Download and unzip Cornell and OpenSubTitles into data directory  \n",
    "python ./main.py --modelTag 'modelName' \n",
    "\n",
    "Hyperparameters:   \n",
    "--numEpochs=30  \n",
    "--batchSize=256  \n",
    "--learningRate=0.002  \n",
    "--dropout=0.9, help='Dropout rate (keep probabilities)  \n",
    "\n",
    "The model was trained using the course Docker container (minor package updates) on an AWS p2.xlarge GPU instance (NVIDIA K80 GPU). 30 epochs took ~2 hours. Tried training on Macbook Pro, but each epoch took 1 hour and 10 minutes. \n",
    "\n",
    "##### Testing \n",
    "python ./main.py --modelTag 'modelName' --test interactive  \n",
    "\n",
    "##### Telgram Chatbot \n",
    "@jay_iqabot   \n",
    "bot name: iQABot\n",
    "\n",
    "Provide the code for training your conversational model and also an inference function, that gets a question and generates an answer.\n",
    "https://github.com/jayurbain/Advanced-Machine-Learning/natural-language-processing/deep_qa\n",
    "\n",
    "#### Provide analysis of the quality: give some examples of good and bad responses. Could you identify some common problems of your bot?\n",
    "\n",
    "Sample conversaton using Telegram:  \n",
    "Q: What is your name? # ok response  \n",
    "A: Bedever, my liege.\n",
    "\n",
    "Q: How old are you? # ok response  \n",
    "A: Twenty-five.\n",
    "\n",
    "Q: Do you like me? # great response\n",
    "A: Yes, I like you.\n",
    "\n",
    "Q: Are you a man? # ok response  \n",
    "A: Yes\n",
    "\n",
    "Q: Are you a woman? # interesting response  \n",
    "A: Sure\n",
    "\n",
    "Q: What is the meaning of life? # weird  \n",
    "A: It's a whale in the bar.\n",
    "\n",
    "Q: Do you love me? # ok response  \n",
    "A: Of course.\n",
    "\n",
    "Q: Do you like dogs?  # bad response  \n",
    "A: Look, i'll be fine.\n",
    "\n",
    "Q: Do you like pizza? # bad response  \n",
    "A: I don't believe that.\n",
    "\n",
    "Q: Jay (my name) # bad, but interesting response  \n",
    "A: I'm sorry Bob\n",
    "\n",
    "Notes: needs better diaglogue management. Does not remember context of conversation. Attention model may help or higher level diaglogue management.\n",
    "\n",
    "#### It's fine to use a starter code from some external source, but in this case you must provide the source links. Also, try at least some modifications of the original code and describe that in the report.  \n",
    "https://www.tensorflow.org/tutorials/seq2seq  \n",
    "https://github.com/gunthercox/ChatterBot  \n",
    "https://github.com/Conchylicultor/DeepQA/blob/master/chatbot/chatbot.py  \n",
    "\n",
    "#### Requirements for the bot:\n",
    "\n",
    "It should be up and running, ready for a small talk! :)   \n",
    "Check!\n",
    "\n",
    "It should recognize at least some grammar rules (e.g. answering with 10 nouns in a row is not fine) and it should answer with somewhat meaningful sentences.  \n",
    "Check!  \n",
    "\n",
    "It's OK if sometimes the responds are weird. However, the responses should be different for different questions.  \n",
    "Check!  \n",
    "\n",
    "Try common questions like \"How are you?\" or \"What is your name?\" and see whether the bot provides adequate answers.  \n",
    "Check!  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conduct some experiments, which confirm that the implemented approach works for the task. Provide these results in the notebook.\n",
    "\n",
    "Output from AWS server responding to questions.\n",
    "\n",
    "Output of command line interactive mode."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Output from AWS server responding to questions.\n",
    "\n",
    "Query: Are you faithful? [101, 5, 22, 97, 283, 5, 2, 5, 2, 5, 2, 5] No. i'm fine.\n",
    "generate answer - question: Are you faithful?  answer:  No. i'm fine.\n",
    "An update received.\n",
    "Update content: {'update_id': 991687723, 'message': {'message_id': 67, 'chat': {'last_name': 'Urbain', 'id': 564604848, 'type': 'private', 'first_name': 'Jay'}, 'from': {'last_name': 'Urbain', 'id': 564604848, 'language_code': 'en-us', 'first_name': 'Jay', 'is_bot': False}, 'text': 'Are you sick?', 'date': 1529777566}}\n",
    "Query: Are you sick? [101, 5, 2, 48, 757, 5, 2, 5, 2, 5, 2, 5] No.\n",
    "generate answer - question: Are you sick?  answer:  No.\n",
    "An update received.\n",
    "Update content: {'update_id': 991687724, 'message': {'message_id': 69, 'chat': {'last_name': 'Urbain', 'id': 564604848, 'type': 'private', 'first_name': 'Jay'}, 'from': {'last_name': 'Urbain', 'id': 564604848, 'language_code': 'en-us', 'first_name': 'Jay', 'is_bot': False}, 'text': 'Are you shot', 'date': 1529777629}}\n",
    "Query: Are you shot [483, 117, 5, 2, 375, 40, 5, 2, 5, 2, 18, 2] Uh huh.\n",
    "generate answer - question: Are you shot  answer:  Uh huh.\n",
    "An update received.\n",
    "Update content: {'update_id': 991687725, 'message': {'message_id': 71, 'chat': {'last_name': 'Urbain', 'id': 564604848, 'type': 'private', 'first_name': 'Jay'}, 'from': {'last_name': 'Urbain', 'id': 564604848, 'language_code': 'en-us', 'first_name': 'Jay', 'is_bot': False}, 'text': 'Drop your gun', 'date': 1529777648}}\n",
    "Query: Drop your gun [22, 400, 55, 75, 1133, 5, 2, 5, 2, 5, 2, 5] I told you something wrong.\n",
    "generate answer - question: Drop your gun  answer:  I told you something wrong.\n",
    "An update received.\n",
    "Update content: {'update_id': 991687726, 'message': {'message_id': 73, 'chat': {'last_name': 'Urbain', 'id': 564604848, 'type': 'private', 'first_name': 'Jay'}, 'from': {'last_name': 'Urbain', 'id': 564604848, 'language_code': 'en-us', 'first_name': 'Jay', 'is_bot': False}, 'text': \"Don't do it?\", 'date': 1529777671}}\n",
    "Query: Don't do it? [101, 66, 22, 46, 24, 5, 2, 5, 2, 5, 2, 5] No, i do n't.\n",
    "generate answer - question: Don't do it?  answer:  No, i do n't.\n",
    "An update received.\n",
    "Update content: {'update_id': 991687727, 'message': {'message_id': 75, 'chat': {'last_name': 'Urbain', 'id': 564604848, 'type': 'private', 'first_name': 'Jay'}, 'from': {'last_name': 'Urbain', 'id': 564604848, 'language_code': 'en-us', 'first_name': 'Jay', 'is_bot': False}, 'text': \"You don't believe that do you?\", 'date': 1529777700}}\n",
    "Query: You don't believe that do you? [101, 5, 2, 48, 33, 414, 221, 5, 2, 5, 2, 5] No.\n",
    "generate answer - question: You don't believe that do you?  answer:  No.\n",
    "\n",
    "\n",
    "\n",
    "# Output of command line interactive mode.\n",
    "(py3.5tf1.5) jays-mbp-2:deep_qa jayurbain$ ./main.py --modelTag pretrainedNMT --test interactive\n",
    "\n",
    "Warning: Restoring parameters:\n",
    "globStep: 32000\n",
    "watsonMode: False\n",
    "autoEncode: False\n",
    "corpus: cornell\n",
    "datasetTag: old\n",
    "maxLength: 10\n",
    "filterVocab: 0\n",
    "skipLines: False\n",
    "vocabularySize: 0\n",
    "hiddenSize: 512\n",
    "numLayers: 2\n",
    "softmaxSamples: 0\n",
    "initEmbeddings: False\n",
    "embeddingSize: 64\n",
    "embeddingSource: None\n",
    "\n",
    "Loading dataset from /Users/jayurbain/Dropbox/Coursera/Advanced Machine Learning/natural-language-processing/deep_qa/data/samples/dataset-cornell-old-length10-filter0-vocabSize0.pkl\n",
    "Loaded cornell: 34991 words, 139979 QA\n",
    "Model creation...\n",
    "Initialize variables...\n",
    "WARNING: Restoring previous model from /Users/jayurbain/Dropbox/Coursera/Advanced Machine Learning/natural-language-processing/deep_qa/save/model-pretrainedv2/model.ckpt\n",
    "Testing: Launch interactive mode:\n",
    "\n",
    "Welcome to the interactive mode, here you can ask to Deep Q&A the sentence you want. Don't have high expectation. Type 'exit' or just press ENTER to quit the program. Have fun.\n",
    "Q: What is your name?\n",
    "A: Bedevere, my liege.\n",
    "\n",
    "Q: How old are you?\n",
    "A: Twenty-five.\n",
    "\n",
    "Q: Do you like me?\n",
    "A: Yes, i like you.\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3.6tf1.3keras]",
   "language": "python",
   "name": "conda-env-py3.6tf1.3keras-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
